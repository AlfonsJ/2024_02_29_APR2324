{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2023_01_30_Cuestión 2.** $\\;$ Considera la siguiente modificación de la función de Widrow y Hoff \n",
    "$$q_S(\\mathbf{\\Theta}) = \\sum_{n=1}^N \\left(\\mathbf{\\Theta}^t\\boldsymbol{x}_n - y_n \\right) + \\lambda~\\mathbf{\\Theta}^t\\sum_{n=1}^N \\boldsymbol{x}_n$$\n",
    "Al aplicar la técnica de descenso por gradiente, en la iteración $k$ el vector de pesos, $\\mathbf{\\Theta}$, se modifica como:\n",
    "$$\\mathbf{\\Theta}(k+1)=\\mathbf{\\Theta}(k)-\\rho_k\\mathbf{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$$\n",
    "En esta expresión, el gradiente, $\\mathbf{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$, es:\n",
    "1. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\lambda~\\mathbf{\\Theta}(k)$\n",
    "2. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\lambda$\n",
    "3. $\\sum_{n=1}^N ~ \\mathbf{\\Theta}(k)^t\\boldsymbol{x}_n  + \\lambda $\n",
    "4. $(1+\\lambda)~ \\sum_{n=1}^N ~ \\boldsymbol{x}_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ la 4; asumiendo $\\,\\mathbf{\\Theta}, \\boldsymbol{x}_n\\in\\mathbb{R}^D\\;$ y que escribimos el gradiente como vector columna\n",
    "$$\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\mathbf{\\Theta}}%\n",
    "=\\begin{bmatrix}\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_1}\\\\\\vdots\\\\\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_D}\\end{bmatrix}%\n",
    "=(1+\\lambda)\\sum_n\\boldsymbol{x}_n%\n",
    "\\qquad\\text{pues}\\quad%\n",
    "\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_d}=\\sum_n x_{nd}+\\lambda\\sum_nx_{nd}=(1+\\lambda)\\sum_n x_{nd}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2023_01_16_Cuestión 3.** $\\;$ Considera la siguiente modificación de la función de Widrow y Hoff \n",
    "$$q_S(\\mathbf{\\Theta}) = \\sum_{n=1}^N\\left(\\mathbf{\\Theta}^t\\boldsymbol{x}_n - y_n \\right) + \\frac{\\lambda}{2}~\\mathbf{\\Theta}, $$\n",
    "Al aplicar la técnica de descenso por gradiente, en la iteración $k$ el vector de pesos, $\\mathbf{\\Theta}$, se modifica como:\n",
    "$$\\mathbf{\\Theta}(k+1)=\\mathbf{\\Theta}(k)-\\rho_k\\mathbf{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$$\n",
    "En esta expresión, el gradiente, $\\mathbf{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$, es:\n",
    "1. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n  + 1$\n",
    "2. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\lambda~\\mathbf{\\Theta}(k)$\n",
    "3. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\frac{\\lambda}{2}$\n",
    "4. $\\sum_{n=1}^N ~ \\mathbf{\\Theta}(k)^t\\boldsymbol{x}_n + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ la 3, aunque le falta un $\\boldsymbol{1}_D$; asumiendo $\\,\\mathbf{\\Theta}, \\boldsymbol{x}_n\\in\\mathbb{R}^D\\;$ y que escribimos el gradiente como vector columna\n",
    "$$\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\mathbf{\\Theta}}%\n",
    "=\\begin{bmatrix}\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_1}\\\\\\vdots\\\\\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_D}\\end{bmatrix}%\n",
    "=\\sum_n\\boldsymbol{x}_n+\\boldsymbol{1}_D\\frac{\\lambda}{2}%\n",
    "\\qquad\\text{pues}\\quad%\n",
    "\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_d}=\\sum_n x_{nd}+\\frac{\\lambda}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2022_01_10_Cuestión 2.** Considera la siguiente modificación de la función de Widrow y Hoff \n",
    "$$q_S(\\mathbf{\\Theta}) = \\sum_{n=1}^N \\left(\\mathbf{\\Theta}^t\\boldsymbol{x}_n - y_n\\right) + \\lambda~\\mathbf{\\Theta}^t\\mathbf{\\Theta}~\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)^t\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)$$\n",
    "Al aplicar la técnica de descenso por gradiente, en la iteración $k$ el vector de pesos, $\\mathbf{\\Theta}$, se modifica como\n",
    "$$\\mathbf{\\Theta}(k+1)=\\mathbf{\\Theta}(k)-\\rho_k\\boldsymbol{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$$\n",
    "En esta expresión, el gradiente, $\\boldsymbol{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$, es:\n",
    "1. $\\displaystyle\\sum_{n=1}^N ~ \\boldsymbol{x}_n +\\lambda~\\mathbf{\\Theta}(k)~\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)^t\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)$\n",
    "2. $\\displaystyle\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\lambda~\\mathbf{\\Theta}(k)$\n",
    "3. $\\displaystyle\\sum_{n=1}^N ~ \\boldsymbol{x}_n +2~\\lambda~\\mathbf{\\Theta}(k)~\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)^t\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)$\n",
    "4. $\\displaystyle\\sum_{n=1}^N ~ \\mathbf{\\Theta}(k)^t\\boldsymbol{x}_n + \\lambda~ \\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)^t\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ la 3, asumiendo $\\,\\mathbf{\\Theta}, \\boldsymbol{x}_n\\in\\mathbb{R}^D\\;$ y que escribimos el gradiente como vector columna\n",
    "$$\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\mathbf{\\Theta}}%\n",
    "=\\begin{bmatrix}\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_1}\\\\\\vdots\\\\\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_D}\\end{bmatrix}%\n",
    "=\\sum_n\\boldsymbol{x}_n+2\\lambda\\boldsymbol{\\Theta}~\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)^t\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)%\n",
    "\\qquad\\text{pues}\\quad%\n",
    "\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_d}=\\sum_n x_{nd}+2\\lambda\\Theta_d~\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)^t\\left(\\sum_{n=1}^N \\boldsymbol{x}_n\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2021_02_03_Cuestión 3.** Se desea ajustar por mínimos cuadrados la función $f:\\mathbb{R}\\rightarrow\\mathbb{R}$, definida como $\\,y=f(x)= ax^2 + bx + c,\\,$ a una secuencia de $N$ pares entrada-salida $\\,S=((x_1,y_1),(x_2,y_2),\\dotsc,(x_N,y_N))$. La técnica empleada es minimizar por descenso por gradiente la función de error cuadrático:\n",
    "$$ q(a,b,c) = \\sum_{n=1}^N (f(x_n)-y_n)^2 $$\n",
    "Identifica la afirmación acertada de entre las siguientes:\n",
    "1. El gradiente es $2ax + b$\n",
    "2. El descenso por gradiente solo es aplicable a funciones convexas, pero $q(\\cdot)$ no lo es\n",
    "3. La técnica de descenso por gradiente no es aplicable en este caso ya que la función a ajustar, $f(\\cdot)$, no es lineal\n",
    "4. El gradiente es: $2\\sum_{n=1}^N (f(\\boldsymbol{x}_n)-y_n)~\\left[~x^2_n, x_n, 1\\right]^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** descenso por gradiente puede aplicarse a funciones derivables en general; la 4 es correcta\n",
    "$$\\dfrac{\\partial q}{\\partial(a, b, c)^t}%\n",
    "=\\begin{pmatrix}\\sum_n 2(f(x_n)-y_n)x_n^2\\\\\\sum_n 2(f(x_n)-y_n)x_n\\\\\\sum_n 2(f(x_n)-y_n)\\end{pmatrix}\n",
    "=2\\sum_n (f(x_n)-y_n)(x_n^2, x_n, 1)^t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2021_01_18_Cuestión 4.** Considera la siguiente modificación de la función de Widrow y Hoff \n",
    "$$q_S(\\mathbf{\\Theta}) = \\sum_{n=1}^N \\left(\\mathbf{\\Theta}^t\\boldsymbol{x}_n - y_n \\right) + \\frac{\\lambda}{2}~\\mathbf{\\Theta}$$\n",
    "Al aplicar la técnica de descenso por gradiente, en la iteración $k$ el vector de pesos, $\\mathbf{\\Theta}$, se modifica como:\n",
    "$$\\mathbf{\\Theta}(k+1)=\\mathbf{\\Theta}(k)-\\rho_k\\boldsymbol{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$$\n",
    "En esta expresión, el gradiente, $\\boldsymbol{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$, es:\n",
    "1. $\\displaystyle \\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\frac{\\lambda}{2}$\n",
    "2. $\\displaystyle \\sum_{n=1}^N ~ \\boldsymbol{x}_n  + 1$\n",
    "3. $\\displaystyle \\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\lambda~\\mathbf{\\Theta}(k)$\n",
    "4. $\\displaystyle \\sum_{n=1}^N ~ \\mathbf{\\Theta}(k)^t\\boldsymbol{x}_n + 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ la 1, aunque le falta un $\\boldsymbol{1}_D$; asumiendo $\\,\\mathbf{\\Theta}, \\boldsymbol{x}_n\\in\\mathbb{R}^D\\;$ y que escribimos el gradiente como vector columna\n",
    "$$\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\mathbf{\\Theta}}%\n",
    "=\\begin{bmatrix}\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_1}\\\\\\vdots\\\\\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_D}\\end{bmatrix}%\n",
    "=\\sum_n\\boldsymbol{x}_n+\\boldsymbol{1}_D\\frac{\\lambda}{2}%\n",
    "\\qquad\\text{pues}\\quad%\n",
    "\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_d}=\\sum_n x_{nd}+\\frac{\\lambda}{2}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
