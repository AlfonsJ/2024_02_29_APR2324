{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2023_01_30_Cuestión 2.** $\\;$ Considera la siguiente modificación de la función de Widrow y Hoff \n",
    "$$q_S(\\mathbf{\\Theta}) = \\sum_{n=1}^N \\left(\\mathbf{\\Theta}^t\\boldsymbol{x}_n - y_n \\right) + \\lambda~\\mathbf{\\Theta}^t\\sum_{n=1}^N \\boldsymbol{x}_n$$\n",
    "Al aplicar la técnica de descenso por gradiente, en la iteración $k$ el vector de pesos, $\\mathbf{\\Theta}$, se modifica como:\n",
    "$$\\mathbf{\\Theta}(k+1)=\\mathbf{\\Theta}(k)-\\rho_k\\mathbf{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$$\n",
    "En esta expresión, el gradiente, $\\mathbf{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$, es:\n",
    "1. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\lambda~\\mathbf{\\Theta}(k)$\n",
    "2. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\lambda$\n",
    "3. $\\sum_{n=1}^N ~ \\mathbf{\\Theta}(k)^t\\boldsymbol{x}_n  + \\lambda $\n",
    "4. $(1+\\lambda)~ \\sum_{n=1}^N ~ \\boldsymbol{x}_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ la 4; asumiendo $\\,\\mathbf{\\Theta}, \\boldsymbol{x}_n\\in\\mathbb{R}^D\\;$ y que escribimos el gradiente como vector columna\n",
    "$$\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\mathbf{\\Theta}}%\n",
    "=\\begin{bmatrix}\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_1}\\\\\\vdots\\\\\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_D}\\end{bmatrix}%\n",
    "=(1+\\lambda)\\sum_n\\boldsymbol{x}_n%\n",
    "\\qquad\\text{pues}\\quad%\n",
    "\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_d}=\\sum_n x_{nd}+\\lambda\\sum_nx_{nd}=(1+\\lambda)\\sum_n x_{nd}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2023_01_16_Cuestión 3.** $\\;$ Considera la siguiente modificación de la función de Widrow y Hoff \n",
    "$$q_S(\\mathbf{\\Theta}) = \\sum_{n=1}^N\\left(\\mathbf{\\Theta}^t\\boldsymbol{x}_n - y_n \\right) + \\frac{\\lambda}{2}~\\mathbf{\\Theta}, $$\n",
    "Al aplicar la técnica de descenso por gradiente, en la iteración $k$ el vector de pesos, $\\mathbf{\\Theta}$, se modifica como:\n",
    "$$\\mathbf{\\Theta}(k+1)=\\mathbf{\\Theta}(k)-\\rho_k\\mathbf{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$$\n",
    "En esta expresión, el gradiente, $\\mathbf{\\nabla} q_S(\\mathbf{\\Theta})|_{\\mathbf{\\Theta}=\\mathbf{\\Theta}(k)}$, es:\n",
    "1. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n  + 1$\n",
    "2. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\lambda~\\mathbf{\\Theta}(k)$\n",
    "3. $\\sum_{n=1}^N ~ \\boldsymbol{x}_n + \\frac{\\lambda}{2}$\n",
    "4. $\\sum_{n=1}^N ~ \\mathbf{\\Theta}(k)^t\\boldsymbol{x}_n + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ la 3, aunque le falta un $\\boldsymbol{1}_D$; asumiendo $\\,\\mathbf{\\Theta}, \\boldsymbol{x}_n\\in\\mathbb{R}^D\\;$ y que escribimos el gradiente como vector columna\n",
    "$$\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\mathbf{\\Theta}}%\n",
    "=\\begin{bmatrix}\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_1}\\\\\\vdots\\\\\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_D}\\end{bmatrix}%\n",
    "=\\sum_n\\boldsymbol{x}_n+\\boldsymbol{1}_D\\frac{\\lambda}{2}%\n",
    "\\qquad\\text{pues}\\quad%\n",
    "\\dfrac{\\partial q_S(\\mathbf{\\Theta})}{\\partial\\Theta_d}=\\sum_n x_{nd}+\\frac{\\lambda}{2}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
