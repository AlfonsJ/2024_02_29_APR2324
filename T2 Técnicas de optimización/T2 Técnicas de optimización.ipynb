{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 2. Técnicas de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "[**Introducción a la optimización del PML:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.1%20Introducci%C3%B3n) $\\;$ empezaremos revisando la introducción a la optimización del libro de Kevin Murphy \"Probabilistic Machine Learning: An Introduction\" (PML) que ya se ha visto en la asignatura Percepción (PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentarios sobre las transpas\n",
    "\n",
    "**2. Optimización analítica: gradiente**\n",
    "* [**Cálculo matricial:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T07%20Algebra%20lineal/7.8%20C%C3%A1lculo%20matricial) $\\;$ revisaremos brevemente algunos conceptos de cálculo multivariado en notación matricial\n",
    "\n",
    "**3. Optimización con restricciones: multiplicadores de Lagrange y teorema Kuhn-Tucker**\n",
    "* [**Optimización con restricciones:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.5%20Optimizaci%C3%B3n%20con%20restricciones) $\\;$ veremos la introducción a la optimización con restricciones del PML\n",
    "\n",
    "**4. Técnicas de descenso por gradiente**\n",
    "* [**Métodos de primer orden:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.2%20M%C3%A9todos%20de%20primer%20orden) $\\;$ revisaremos la descripción de los métodos de primer orden del PML\n",
    "* **Descenso por gradiente** escoge el neg-gradiente como dirección de descenso\n",
    "* Converge linealmente en problemas convexos; en particular, si el objetivo es (localmente) cuadrático, el ratio de convergencia dependerá del número de condición $\\kappa$ de la Hessiana (más rápidamente cuanto menor $\\kappa$)\n",
    "* En la práctica se aplican heurísticos para acelerar la convergencia en regiones llanas del objetivo (momentum)\n",
    "\n",
    "**5. Esperanza-Maximización (EM)**\n",
    "* [**Modelos de mixtura:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T03%20Probabilidad%3A%20modelos%20multivariados/3.5%20Modelos%20de%20mixtura) $\\;$ primero veremos las secciones 3.5.0 y 3.5.1 del PML sobre mixturas de Gaussianas\n",
    "* [**Optimización acotada:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada) $\\;$ veremos parte de la sección 8.7 del PML sobre optimización acotada\n",
    "    * [**Introducción:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.0%20Introducci%C3%B3n.ipynb) $\\;$ el EM es un caso particular del MM\n",
    "    * [**El algoritmo general:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.1%20El%20algoritmo%20general.ipynb) $\\;$ algoritmo MM\n",
    "    * [**El algoritmo EM:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.2%20El%20algoritmo%20EM.ipynb) $\\;$ algoritmo EM general\n",
    "    * [**EM para una GMM:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.3%20EM%20para%20una%20GMM.ipynb) $\\;$ veremos las secciones 8.7.3.1, 8.7.3.2 y 8.7.3.3\n",
    "* El algoritmo EM es un caso particular del algoritmo majorize-minimize (MM) para calcular el estimador MLE (o MAP) de modelos probabilísticos con datos perdidos o variables ocultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios de seguimiento propuestos\n",
    "\n",
    "### Ejercicio sobre descenso por gradiente aplicado a regresión logística\n",
    "\n",
    "Considera la NLL para un modelo de regresión logística\n",
    "$$\\operatorname{NLL}(\\mathbf{W})%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_n)\n",
    "\\qquad\\text{con}\\quad%\n",
    "\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)%\n",
    "\\quad\\text{y}\\quad%\n",
    "\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$$\n",
    "Demuestra el siguiente resultado:\n",
    "$$\\begin{pmatrix}%\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial W_{11}}&\\cdots&\\frac{\\partial\\operatorname{NLL}}{\\partial W_{1C}}\\\\%\n",
    "\\vdots&\\ddots&\\vdots\\\\%\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial W_{D1}}&\\cdots&\\frac{\\partial\\operatorname{NLL}}{\\partial W_{DC}}\\\\%\n",
    "\\end{pmatrix}\n",
    "=\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\frac{\\partial(-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_n))}{\\partial\\mathbf{W}^t}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
