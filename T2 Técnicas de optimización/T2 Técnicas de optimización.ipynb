{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 2. Técnicas de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "[**Introducción a la optimización del PML:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.1%20Introducci%C3%B3n) $\\;$ empezaremos revisando la introducción a la optimización del libro de Kevin Murphy \"Probabilistic Machine Learning: An Introduction\" (PML) que ya se ha visto en la asignatura Percepción (PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentarios sobre las transpas\n",
    "\n",
    "**2. Optimización analítica: gradiente**\n",
    "* [**Cálculo matricial:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T07%20Algebra%20lineal/7.8%20C%C3%A1lculo%20matricial) $\\;$ revisaremos brevemente algunos conceptos de cálculo multivariado en notación matricial\n",
    "\n",
    "**3. Optimización con restricciones: multiplicadores de Lagrange y teorema Kuhn-Tucker**\n",
    "* [**Optimización con restricciones:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.5%20Optimizaci%C3%B3n%20con%20restricciones) $\\;$ veremos la introducción a la optimización con restricciones del PML\n",
    "\n",
    "**4. Técnicas de descenso por gradiente**\n",
    "* [**Métodos de primer orden:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.2%20M%C3%A9todos%20de%20primer%20orden) $\\;$ revisaremos la descripción de los métodos de primer orden del PML\n",
    "* **Descenso por gradiente** escoge el neg-gradiente como dirección de descenso\n",
    "* Converge linealmente en problemas convexos; en particular, si el objetivo es (localmente) cuadrático, el ratio de convergencia dependerá del número de condición $\\kappa$ de la Hessiana (más rápidamente cuanto menor $\\kappa$)\n",
    "* En la práctica se aplican heurísticos para acelerar la convergencia en regiones llanas del objetivo (momentum)\n",
    "\n",
    "**5. Esperanza-Maximización (EM)**\n",
    "* [**Optimización acotada:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada) $\\;$ veremos parte de la sección 8.7 del PML sobre optimización acotada\n",
    "    * [**Introducción:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.0%20Introducci%C3%B3n.ipynb) $\\;$ el EM es un caso particular del MM\n",
    "    * [**El algoritmo general:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.1%20El%20algoritmo%20general.ipynb) $\\;$ algoritmo MM\n",
    "    * [**El algoritmo EM:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.2%20El%20algoritmo%20EM.ipynb) $\\;$ algoritmo EM general\n",
    "    * [**EM para una GMM:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.3%20EM%20para%20una%20GMM.ipynb) $\\;$ veremos las secciones 8.7.3.1, 8.7.3.2 y 8.7.3.3\n",
    "* El algoritmo EM es un caso particular del algoritmo majorize-minimize (MM) para calcular el estimador MLE (o MAP) de modelos probabilísticos con datos perdidos o variables ocultas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
