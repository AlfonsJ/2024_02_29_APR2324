{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 2. Técnicas de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "[**Introducción a la optimización del PML:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.1%20Introducci%C3%B3n) $\\;$ empezaremos revisando la introducción a la optimización del libro de Kevin Murphy \"Probabilistic Machine Learning: An Introduction\" (PML) que ya se ha visto en la asignatura Percepción (PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentarios sobre las transpas\n",
    "\n",
    "**2. Optimización analítica: gradiente**\n",
    "* [**Cálculo matricial:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T07%20Algebra%20lineal/7.8%20C%C3%A1lculo%20matricial) $\\;$ revisaremos brevemente algunos conceptos de cálculo multivariado en notación matricial\n",
    "\n",
    "**3. Optimización con restricciones: multiplicadores de Lagrange y teorema Kuhn-Tucker**\n",
    "* [**Optimización con restricciones:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.5%20Optimizaci%C3%B3n%20con%20restricciones) $\\;$ veremos la introducción a la optimización con restricciones del PML\n",
    "\n",
    "**4. Técnicas de descenso por gradiente**\n",
    "* [**Métodos de primer orden:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.2%20M%C3%A9todos%20de%20primer%20orden) $\\;$ revisaremos la descripción de los métodos de primer orden del PML\n",
    "* **Descenso por gradiente** escoge el neg-gradiente como dirección de descenso\n",
    "* Converge linealmente en problemas convexos; en particular, si el objetivo es (localmente) cuadrático, el ratio de convergencia dependerá del número de condición $\\kappa$ de la Hessiana (más rápidamente cuanto menor $\\kappa$)\n",
    "* En la práctica se aplican heurísticos para acelerar la convergencia en regiones llanas del objetivo (momentum)\n",
    "\n",
    "**5. Esperanza-Maximización (EM)**\n",
    "* [**Modelos de mixtura:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T03%20Probabilidad%3A%20modelos%20multivariados/3.5%20Modelos%20de%20mixtura) $\\;$ primero veremos las secciones 3.5.0 y 3.5.1 del PML sobre mixturas de Gaussianas\n",
    "* [**Optimización acotada:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada) $\\;$ veremos parte de la sección 8.7 del PML sobre optimización acotada\n",
    "    * [**Introducción:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.0%20Introducci%C3%B3n.ipynb) $\\;$ el EM es un caso particular del MM\n",
    "    * [**El algoritmo general:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.1%20El%20algoritmo%20general.ipynb) $\\;$ algoritmo MM\n",
    "    * [**El algoritmo EM:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.2%20El%20algoritmo%20EM.ipynb) $\\;$ algoritmo EM general\n",
    "    * [**EM para una GMM:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P01%20Fundamentos/T08%20Optimizaci%C3%B3n/8.7%20Optimizaci%C3%B3n%20acotada/8.7.3%20EM%20para%20una%20GMM.ipynb) $\\;$ veremos las secciones 8.7.3.1, 8.7.3.2 y 8.7.3.3\n",
    "* El algoritmo EM es un caso particular del algoritmo majorize-minimize (MM) para calcular el estimador MLE (o MAP) de modelos probabilísticos con datos perdidos o variables ocultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios de seguimiento propuestos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio sobre descenso por gradiente aplicado a la función Rosenbrock\n",
    "\n",
    "Considera la aplicación de descenso por gradiente a la [**función Rosenbrock**](https://en.wikipedia.org/wiki/Rosenbrock_function), $\\,f(x, y)=(a-x)^2+b(y-x^2)^2,\\,$ con $\\,a=1,\\,$ $b=10\\,$ y punto de inicio $\\,(x_0,y_0)^t=(-1, 1).$ \n",
    "Estudia el número de iteraciones hasta convergencia (si converge) en función del factor de aprendizaje, sin y con momentum.\n",
    "Opcionalmente, extiende el estudio considerando puntos de inicio distintos, así como otros valores de $a$ y $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensión opcional del ejercicio anterior\n",
    "\n",
    "La función Rosenbrock es una de muchas [**funciones de test para optimización**](https://en.wikipedia.org/wiki/Test_functions_for_optimization) utilizadas con el fin de estudiar el comportamiento de algoritmos de optimización. Opcionalmente, si quieres extender el estudio realizado en el ejercicio anterior, repítelo con funciones de test para optimización de un objetivo único descritas en el enlace anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio sobre descenso por gradiente aplicado a regresión logística\n",
    "\n",
    "La NLL para un modelo de regresión logística es\n",
    "$$\\operatorname{NLL}(\\mathbf{W})%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_n)\n",
    "\\qquad\\text{con}\\quad%\n",
    "\\boldsymbol{\\mu}_n=\\mathcal{S}(\\boldsymbol{a}_n)%\n",
    "\\quad\\text{y}\\quad%\n",
    "\\boldsymbol{a}_n=\\mathbf{W}^t\\boldsymbol{x}_n$$\n",
    "Demuestra el siguiente resultado:\n",
    "$$\\begin{pmatrix}%\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial W_{11}}&\\cdots&\\frac{\\partial\\operatorname{NLL}}{\\partial W_{1C}}\\\\%\n",
    "\\vdots&\\ddots&\\vdots\\\\%\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial W_{D1}}&\\cdots&\\frac{\\partial\\operatorname{NLL}}{\\partial W_{DC}}\\\\%\n",
    "\\end{pmatrix}\n",
    "=\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\frac{\\partial(-\\log p(\\boldsymbol{y}_n\\mid\\boldsymbol{\\mu}_n))}{\\partial\\mathbf{W}^t}%\n",
    "=\\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
