{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2023_01_30_Cuestión 3.** $\\;$ Las siguientes afirmaciones se refieren a la estimación por máxima verosimilitud de los parámetros de una mezcla de $K$ gaussianas (vector-media y peso de cada gaussiana) mediante un conjunto de vectores de entrenamiento cualquiera de dimensión $D$. Identifica cuál es **falsa**.\n",
    "1. Los parámetros de la mezcla se estiman adecuadamente mediante un algoritmo de **esperanza maximización** (EM)\n",
    "2. La verosimilitud del conjunto de entrenamiento, calculada con los parámetros estimados, aumenta en cada iteración del EM\n",
    "3. El algoritmo EM obtiene los valores óptimos de los parámetros a estimar\n",
    "4. En cada iteración, el algoritmo EM estima los valores de las variables ocultas que, en este caso, son los pesos de las gaussianas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ La 3 es falsa ya que el EM encuentra un óptimo local, no necesariamente global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2022_01_24_Cuestión 3.** $\\;$ En el problema de aprendizaje de modelos probabilísticos con variables observables $\\boldsymbol{x}_n$ y latentes $\\boldsymbol{z}_n$, la log-verosimilitud se expresa como:\n",
    "$$L_S(\\mathbf{\\Theta}) ~=~\\sum_{n=1}^N\\log \\left(\\sum_{\\boldsymbol{z}_n} P(\\boldsymbol{x}_n,\\boldsymbol{z}_n\\mid\\mathbf{\\Theta}) \\right)$$\n",
    "y se utiliza la técnica esperanza-maximización (EM).  Indicar cuál de las siguientes afirmaciones es cierta:\n",
    "1. En cada iteración, el paso E consiste en obtener una estimación de todas las variables $\\boldsymbol{x}_n$ y $\\boldsymbol{z}_n$, y el paso M,obtener los parámetros óptimos de $\\mathbf{\\Theta}$ utilizando la estimación de las variables $\\boldsymbol{x}_n$ y $\\boldsymbol{z}_n$ obtenidas en el paso E.\n",
    "2. En cada iteración, el paso E consiste en obtener una estimación de los valores de las variables latentes $\\boldsymbol{z}_n$, y el paso M, obtener los parámetros óptimos de $\\mathbf{\\Theta}$ utilizando los estimadores de $\\boldsymbol{z}_n$ obtenidas en el paso E.\n",
    "3. En cada iteración, el paso E consiste en obtener los valores de las variables latentes $\\boldsymbol{z}_n$ que maximizan la función objetivo $L_S(\\mathbf{\\Theta})$, y el paso M, obtener los parámetros óptimos de $\\mathbf{\\Theta}$ utilizando la estimación de las variables $\\boldsymbol{z}_n$ obtenidas en el paso E.\n",
    "4. En cada iteración, el paso E consiste en obtener los valores de todas las variables $\\boldsymbol{x}_n$ y $\\boldsymbol{z}_n$ que maximizan la función $L_S(\\mathbf{\\Theta})$, y el paso M, obtener los parámetros óptimos de $\\mathbf{\\Theta}$ utilizando la estimación de las variables $\\boldsymbol{x}_n$ y $\\boldsymbol{z}_n$ obtenidas en el paso E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ La 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2022_01_10_Cuestión 4.** $\\;$ En el aprendizaje de modelos probabilísticos mediante el algoritmo esperanza-maximización indicar qué afirmación es correcta.\n",
    "1. Es el método adecuado cuando las muestras de entrenamiento están completas.\n",
    "2. A partir de una cierta estimación de las variables ocultas se busca el óptimo de una función auxiliar como solución final del problema original.\n",
    "3. Es el método adecuado cuando no hay variables ocultas.\n",
    "4. En cada iteración, a partir de una cierta estimación de las variables ocultas se busca el óptimo de una función auxiliar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ La 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2021_02_03_Cuestión 2.** $\\;$ En la estimación por máxima verosimilitud de los parámetros de una mezcla de $K$ gaussianas de matriz de covarianza común y conocida a partir de $N$ vectores de entrenamiento, los parámetros a estimar son: el vector-media $\\boldsymbol{\\mu}_k$ y el peso $\\alpha_k$ de cada gaussiana, $k, 1\\leq k\\leq K$. Identificar cuál de las siguientes afirnaciones es **correcta:**\n",
    "1. El método más adecuado es el de **esperanza-maximización** (EM), el cual garantiza que se cumple la restricción $\\sum_{k=1}^K\\alpha_k=1$.  Esto es así gracias a que, en cada iteración de EM, los valores de $\\alpha_k, 1\\leq k\\leq K$, se obtienen como medias de valores de variables latentes, usando una expresión que se deriva analíticamente mediante la técnica de los **multiplicadores de Lagrange** con la restricción indicada.\n",
    "2. Se puede usar **descenso por gradiente,** ya que los valores de $\\boldsymbol{\\mu}_k$ no están sujetos a ninguna restricción, lo que hace innecesario recurrir a la técnica de los **multiplicadores de Lagrange.**\n",
    "3. La solución se obtiene en un paso, utilizando directamente la **optimización lagrangiana** de la verosimilitud de los $N$ vectores de entrenamiento.  En este caso, hay un único multiplicador de Lagrange, $\\beta$, asociado a la restricción de igualdad: $\\sum_{k=1}^K\\alpha_k=1$.\n",
    "4. El método más adecuado sería el de **esperanza-maximización** (EM), pero no es posible utilizarlo ya que EM es un método iterativo que no garantiza el cumplimiento de la restricción de igualdad: $\\sum_{k=1}^K\\pi_k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ La 1."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
