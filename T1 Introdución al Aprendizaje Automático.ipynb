{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 1. Introducción al Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "[**Introducción del PML:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P00%20Introducci%C3%B3n) $\\;$ empezaremos revisando la introducción al Aprendizaje Automático del libro de Kevin Murphy \"Probabilistic Machine Learning: An Introduction\" (PML) que ya se ha visto en la asignatura Percepción (PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentarios sobre las transpas\n",
    "\n",
    "**5. Teoría de la decisión:**\n",
    "* [**Teoría de la decisión Bayesiana:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T05%20Teor%C3%ADa%20de%20la%20decisi%C3%B3n/5.1%20Teor%C3%ADa%20de%20la%20decisi%C3%B3n%20Bayesiana) $\\;$ revisaremos brevemente la introducción a esta teoría del PML, ya vista en PER\n",
    "* En problemas de clasificación, la minimización del riesgo empírico con pérdida 01 conduce al estimador máximo a posteriori (MAP) como regla de decisión óptima o regla de Bayes, esto es, escogemos la etiqueta más probable o moda de la probabilidad a posteriori\n",
    "* En problemas de regresión, la minimización del riesgo empírico con pérdida L2 conduce al estimador minimum mean squared error (MMSE) como regla de decisión óptima o regla de Bayes, esto es, escogemos la media a posteriori\n",
    "\n",
    "**6. Estimación empírica de la probabilidad de error:**\n",
    "* La estimación de la probabilidad de error de un clasificador se refiere a la aproximación de su riesgo teórico con un conjunto de test (Sección 1.2.3 del PML)\n",
    "* Resustitución es el riesgo empírico, mientras que partición, validación cruzada y exclusión individual son técnicas más o menos refinadas para estimar la probabilidad de error de un clasificador ajustado con la evidencia (todos los datos)\n",
    "\n",
    "**7. Aprendizaje estadístico:**\n",
    "* [**Estimación máximo-verosímil:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P01%20Fundamentos/T04%20Estad%C3%ADstica/4.2%20Estimaci%C3%B3n%20m%C3%A1ximo-veros%C3%ADmil) $\\;$ revisaremos brevemente la definición y ejemplos de MLE, ya visto en PER\n",
    "* Aunque en el pasado era habitual ajustar modelos mediante maximización de la verosimilitud conjunta, $p(\\boldsymbol{x},\\boldsymbol{y}\\mid\\boldsymbol{\\theta})$, los modelos actuales y las redes en particular se ajustan mediante maximización de la verosimilitud condicional, $p(\\boldsymbol{y}\\mid\\boldsymbol{x},\\boldsymbol{\\theta})$\n",
    "\n",
    "**8. Sesgo-varianza y sobregeneralización-sobreajuste**\n",
    "* Como se indica en la Sección 1.2.3 del PML, la evaluación del sobreajuste o subajuste de modelos se suele hacer comparando el riesgo empírico con el teórico (ver el ejemplo de regresión polinómica simple con datos sintéticos)\n",
    "* Como se discute en la Sección 4.7.6 del PML, el compromiso sesgo-varianza se plantea bajo una aproximación clásica (frecuentista) a la inferencia estadística, asumiendo una aproximación Gaussiana a la distribución en el muestreo del MLE y que queremos hallar un estimador de mínimo MSE; así, el compromiso sesgo-varianza puede resumirse como \"MSE = varianza + sesgo al cuadrado\"\n",
    "* Del resultado anterior se deduce que podemos preferir un estimador sesgado si, bajo el criterio MSE, la reducción de varianza compensa el incremento de sesgo cuadrático (considera el ejemplo de regresión polinómica simple con datos sintéticos en función del grado del polinomio; también son interesantes los ejemplos de la Sección 4.7.6 del PML)\n",
    "* El compromiso sesgo-varianza para clasificación (con pérdida 01 en lugar del error cuadrático) se considera en la Sección 4.7.6.6 del PML; en breve, no es muy útil\n",
    "\n",
    "**9. La amenaza de la dimensionalidad**\n",
    "* [**The curse of dimensionality:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P04%20Modelos%20no%20param%C3%A9tricos/T16%20M%C3%A9todos%20basados%20en%20ejemplos/16.1%20K-vecinos%20m%C3%A1s%20pr%C3%B3ximos/16.1.2%20La%20maldici%C3%B3n%20de%20la%20dimensionalidad.ipynb) $\\;$ revisaremos brevemente lo que se dijo en PER en el marco del clasificador KNN\n",
    "* La maldición de la dimensionalidad se combate con técnicas de reducción de la dimensión (p.e. PCA) o, más generalmente, con la ingeniería de características propia de las redes neuronales profundas (p.e. en aprendizaje de métricas profundo para el KNN)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
