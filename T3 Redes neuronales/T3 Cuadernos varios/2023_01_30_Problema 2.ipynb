{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023_01_30_Problema 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><table border-collapse: collapse><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=500>\n",
    "\n",
    "El perceptrón multicapa de la figura se utiliza para resolver un problema de regresión.\n",
    "Se asume que la función de activación de los nodos de la capa de salida es de tipo lineal y de la capa oculta es de tipo sigmoid. Sean:\n",
    "* Vector de entrada: $\\;x_1=1.0,\\,x_2=0.0,\\,x_3=2.0$\n",
    "* Salidas de la capa oculta: $\\;s^1_1=0.622,\\,s^1_2=0.119$\n",
    "* Salidas de la capa de salida: $\\;s^2_1=-0.3145,\\,s^2_2=1.4304,\\,s^2_3=-0.9304$\n",
    "* Valores deseados de la capa de salida: $\\;t_1=-1.0,\\,t_2=1.0,\\,t_3=-2.0$\n",
    "\n",
    "Se pide:\n",
    "1. Los correspondientes errores en los tres nodos de la capa de salida y en los dos nodos de la capa oculta.\n",
    "2. Los nuevos valores de los pesos $\\theta^2_{32}$ y $\\theta^1_{23}$ asumiendo que el factor de aprendizaje $\\rho$ es $1.0$\n",
    "\n",
    "</td><td style=\"border: none; padding:0; margin:0;\" width=50><br></td>\n",
    "<td style=\"border: none; text-align:right; vertical-align:top; padding:0; margin:0;\" width=500>\n",
    "\n",
    "<img src=\"2023_01_30_Problema 2.png\" width=500>\n",
    "\n",
    "</td></tr></table></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución \n",
    "$$\\begin{align*}\n",
    "\\delta^2_1&=(t_1-s^2_1)=-0.6855\\\\%\n",
    "\\delta^2_2&=(t_2-s^2_2)=-0.4304\\\\%\n",
    "\\delta^2_3&=(t_3-s^2_3)=-1.0696\\\\%\n",
    "\\delta^1_1&=(\\delta^2_1~\\theta^2_{11}+\\delta^2_2~\\theta^2_{21}+\\delta^2_3~\\theta^2_{31})~s^1_1~(1-s^1_1)=0.3167\\\\%\n",
    "\\delta^1_2&=(\\delta^2_1~\\theta^2_{12}+\\delta^2_2~\\theta^2_{22}+\\delta^2_3~\\theta^2_{32})~s^1_2~(1-s^1_2)=-0.0049\\\\%\n",
    "\\theta^2_{32}&=\\theta^2_{32}+\\rho~\\delta^2_3~s^1_2=-1+1\\cdot -1.0696\\cdot -0.9304=-1.25499\\\\%\n",
    "\\theta^1_{23}&=\\theta^1_{23}+\\rho~\\delta^1_2~x_3=-0.5+1\\cdot -0.0049\\cdot 2.0=-0.5195%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución con notación matricial\n",
    "\n",
    "**Capa oculta:** $\\qquad\\boldsymbol{h}=\\boldsymbol{\\sigma}(\\boldsymbol{z})\\qquad$ con $\\qquad\\boldsymbol{z}=\\mathbf{W}\\boldsymbol{x}+\\boldsymbol{b}_1\\qquad\\mathbf{W}=\\begin{pmatrix}-1.5&-1&0.5\\\\1&1&-0.5\\end{pmatrix}\\qquad\\boldsymbol{b}_1=\\begin{pmatrix}1\\\\-2\\end{pmatrix}$\n",
    "\n",
    "**Capa de salida:** $\\qquad\\hat{\\boldsymbol{y}}=\\mathbf{V}\\boldsymbol{h}+\\boldsymbol{b}_2\\qquad$ con $\\qquad\\mathbf{V}=\\begin{pmatrix}-1.5&1\\\\0.5&1\\\\-0.5&-1\\end{pmatrix}\\qquad\\boldsymbol{b}_2=\\begin{pmatrix}0.5\\\\1\\\\-0.5\\end{pmatrix}$\n",
    "\n",
    "**Pérdida cuadrática:** $\\qquad\\mathcal{L}=\\frac{1}{2}\\lVert\\boldsymbol{y}-\\hat{\\boldsymbol{y}}\\rVert_2^2\\qquad$ para $\\qquad\\boldsymbol{x}=(1,0,2)^t\\qquad\\boldsymbol{y}=(-1,1,-2)^t$\n",
    "\n",
    "**Forward:** $\\;$ pre-activaciones, activaciones y pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = [ 0.5 -2. ]\n",
      "h = [0.6225 0.1192]\n",
      "haty = [-0.3145  1.4304 -0.9304]\n",
      "loss = 0.8996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; np.set_printoptions(precision=4)\n",
    "sigmoid = lambda x: 1.0 / (1.0 + np.exp(-x));\n",
    "x = np.array([1, 0, 2]); y = np.array([-1, 1, -2])\n",
    "W = np.array([[-1.5, -1, 0.5], [1, 1, -0.5]]); b1 = np.array([1, -2])\n",
    "V = np.array([[-1.5, 1], [0.5, 1], [-0.5, -1]]); b2 = np.array([0.5, 1, -0.5])\n",
    "z = (W @ x + b1);                    print(\"z =\", z)\n",
    "h = sigmoid(z);                      print(\"h =\", h)\n",
    "haty = V @ h + b2;                   print(\"haty =\", haty)\n",
    "loss = .5 * np.square(y-haty).sum(); print(\"loss =\", round(loss, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward:** $\\;$ Jacobianas de la pérdida con respecto a $\\ldots$\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{u}^t=&\\dfrac{\\partial\\mathcal{L}}{\\partial\\hat{\\boldsymbol{y}}}=(\\hat{\\boldsymbol{y}}-\\boldsymbol{y})^t\n",
    "&&\\text{la predicción (activación de la capa de salida)}\\\\\n",
    "\\boldsymbol{g}_{\\mathbf{V}}=\\boldsymbol{u}^t&\\dfrac{\\partial\\hat{\\boldsymbol{y}}}{\\partial\\mathbf{V}}=\\boldsymbol{h}\\boldsymbol{u}^t\n",
    "&&\\text{la transformación lineal de la capa de salida}\\\\\n",
    "\\boldsymbol{g}_{\\boldsymbol{b}_2}=\\boldsymbol{u}^t&\\dfrac{\\partial\\hat{\\boldsymbol{y}}}{\\partial\\boldsymbol{b}_2}=\\boldsymbol{u}^t\n",
    "&&\\text{el sesgo de la capa de salida}\\\\\n",
    "\\boldsymbol{u}^t=\\boldsymbol{u}^t&\\dfrac{\\partial\\hat{\\boldsymbol{y}}}{\\partial\\boldsymbol{h}}=\\boldsymbol{u}^t\\mathbf{V}\n",
    "&&\\text{la activación de la capa oculta}\\\\\n",
    "\\boldsymbol{u}^t=\\boldsymbol{u}^t&\\dfrac{\\partial\\boldsymbol{h}}{\\partial\\boldsymbol{z}}=\\boldsymbol{u}^t\\operatorname{diag}(\\boldsymbol{\\sigma}'(\\boldsymbol{z}))\n",
    "&&\\text{la pre-activación de la capa oculta}\\\\\n",
    "\\boldsymbol{g}_{\\mathbf{W}}=\\boldsymbol{u}^t&\\dfrac{\\partial\\boldsymbol{z}}{\\partial\\mathbf{W}}=\\boldsymbol{x}\\boldsymbol{u}^t\n",
    "&&\\text{la transformación lineal de la capa oculta}\\\\\n",
    "\\boldsymbol{g}_{\\boldsymbol{b}_1}=\\boldsymbol{u}^t&\\dfrac{\\partial\\boldsymbol{z}}{\\partial\\boldsymbol{b}_1}=\\boldsymbol{u}^t\n",
    "&&\\text{el sesgo de la capa oculta}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J_haty = [0.6855 0.4304 1.0696]\n",
      "J_V = [[0.4267 0.2679 0.6658]\n",
      " [0.0817 0.0513 0.1275]]\n",
      "J_b2 = [0.6855 0.4304 1.0696]\n",
      "J_h = [-1.3478  0.0464]\n",
      "J_z = [-0.3167  0.0049]\n",
      "J_W = [[-0.3167  0.0049]\n",
      " [-0.      0.    ]\n",
      " [-0.6335  0.0097]]\n",
      "J_b1 = [-0.3167  0.0049]\n"
     ]
    }
   ],
   "source": [
    "J_haty = haty-y;                         print(\"J_haty =\", J_haty)\n",
    "J_V = np.outer(h, J_haty);               print(\"J_V =\", J_V)\n",
    "J_b2 = J_haty;                           print(\"J_b2 =\", J_b2)\n",
    "J_h = J_haty @ V;                        print(\"J_h =\", J_h)\n",
    "J_z = J_h * sigmoid(z) * sigmoid(-z);    print(\"J_z =\", J_z)\n",
    "J_W = np.outer(x, J_z);                  print(\"J_W =\", J_W)\n",
    "J_b1 = J_z;                              print(\"J_b1 =\", J_b1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actualización de parámetros:** $\\quad\\boldsymbol{\\theta}=\\boldsymbol{\\theta}-\\rho\\boldsymbol{g}_{\\boldsymbol{\\theta}}^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V = [[-1.9267  0.9183]\n",
      " [ 0.2321  0.9487]\n",
      " [-1.1658 -1.1275]]\n",
      "b2 = [-0.1855  0.5696 -1.5696]\n",
      "W = [[-1.1833 -1.      1.1335]\n",
      " [ 0.9951  1.     -0.5097]]\n",
      "b1 = [ 1.3167 -2.0049]\n"
     ]
    }
   ],
   "source": [
    "V  = V  - 1.0 * J_V.T; print(\"V =\", V)\n",
    "b2 = b2 - 1.0 * J_b2;  print(\"b2 =\", b2)\n",
    "W  = W  - 1.0 * J_W.T; print(\"W =\", W)\n",
    "b1 = b1 - 1.0 * J_b1;  print(\"b1 =\", b1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
