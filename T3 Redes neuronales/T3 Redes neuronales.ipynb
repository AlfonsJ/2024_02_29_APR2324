{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 3. Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "[**Introducción a las redes neuronales del PML:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P03%20Redes%20neuronales%20profundas/T13%20Redes%20neuronales%20para%20datos%20tabulados/13.1%20Introducci%C3%B3n/13.1%20Introducci%C3%B3n.ipynb) $\\;$ empezaremos revisando la introducción a las redes neuronales del PML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comentarios sobre las transpas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2. Redes neuronales multicapa**\n",
    "* [**Perceptrones multicapa:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P03%20Redes%20neuronales%20profundas/T13%20Redes%20neuronales%20para%20datos%20tabulados/13.2%20Perceptrones%20multicapa/13.2%20Perceptrones%20multicapa%20(31Jul23).ipynb) $\\;$ veremos la sección 13.2 del PML\n",
    "* **Notación APR:** $\\;$ un perceptrón de dos capas\n",
    "\n",
    "<div><table border-collapse: collapse><tr>\n",
    "<td style=\"border: none; padding:0; margin:0;\" width=50><br></td>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=400>\n",
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "\n",
    "**Dinámica**\n",
    "\n",
    "</div>\n",
    "\n",
    "**Capa de entrada:** $\\;1\\leq i\\leq M_0\\equiv d = 5$ \n",
    "$$\\boldsymbol{x}=(x_0,x_1,\\dotsc,x_d)\\in\\mathbb{R}^{d+1}\\quad x_0=1$$\n",
    "\n",
    "**Capa oculta:** $\\;1\\leq i\\leq M_1=3$\n",
    "$$s^1_i(\\boldsymbol{x};\\boldsymbol{\\Theta})=g\\left(\\sum_{j=0}^{M_0} \\theta_{ij}^1~x_j\\right)$$\n",
    "\n",
    "**Capa de salida:** $\\;1\\leq i\\leq M_2=3$\n",
    "$$s^2_i(\\boldsymbol{x};\\boldsymbol{\\Theta})=g\\left(\\sum_{j=0}^{M_1} \\theta_{ij}^2~s^1_j(\\boldsymbol{x})\\right)$$\n",
    "\n",
    "**Parámetros:** $\\;\\{\\theta_{ij}^k\\}$\n",
    "\n",
    "</td><td style=\"border: none; padding:0; margin:0;\" width=50><br></td>\n",
    "<td style=\"border: none; text-align:right; vertical-align:top; padding:0; margin:0;\" width=450>\n",
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: center;\">\n",
    "\n",
    "**Topología**\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=\"Perceptrón de dos capas.png\" width=450>\n",
    "\n",
    "</td></tr></table></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**3. Algoritmo de retropropagación del error (backprop)**\n",
    "* [**Introducción:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P03%20Redes%20neuronales%20profundas/T13%20Redes%20neuronales%20para%20datos%20tabulados/13.3%20Backpropagation/13.3.0%20Introducci%C3%B3n.ipynb) $\\;$ backprop calcula el gradiente de la pérdida con respecto a los parámetros de cada capa\n",
    "* [**Algoritmo backprop:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/blob/main/P03%20Redes%20neuronales%20profundas/T13%20Redes%20neuronales%20para%20datos%20tabulados/13.3%20Backpropagation/13.3.2%20Diferenciaci%C3%B3n%20hacia%20atr%C3%A1s%20para%20MLPs.ipynb) $\\;$ dado un MLP de $K$ capas, backprop consta de dos pasos básicos\n",
    "    * **Forward:** $\\;$ **para** $\\;k=1:K:\\quad\\boldsymbol{x}_{k+1}=\\boldsymbol{f}_k(\\boldsymbol{x}_k,\\boldsymbol{\\theta}_k)$\n",
    "    * **Backward:** $\\;$ **para** $\\;k=K:1:\\quad\\boldsymbol{g}_k=\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\theta}_k}\\quad\\boldsymbol{u}_k^t=\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{x}_k}\\quad$ (cálculo eficiente con la regla de la cadena)\n",
    "        * **Actualización de parámetros:** $\\;\\boldsymbol{\\theta}_k=\\boldsymbol{\\theta}_k+\\mathbf{\\Delta}_k,\\,$ típicamente con $\\,\\mathbf{\\Delta}_k=-\\eta\\boldsymbol{g}_k$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Aspectos de uso y propiedades del backprop**\n",
    "* [**Entrenamiento de redes neuronales:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P03%20Redes%20neuronales%20profundas/T13%20Redes%20neuronales%20para%20datos%20tabulados/13.4%20Entrenamiento%20de%20redes%20neuronales) $\\;$ veremos la sección 13.4 del PML\n",
    "    * Ajuste del factor de aprendizaje: $\\;$ necesario para garantizar la convergencia a una buena solución (en SGD en general)\n",
    "    * El problema de los gradientes que desaparecen y explotan: $\\;$ suele darse al entrenar modelos muy profundos\n",
    "    * Solución para los gradientes que explotan: $\\;$ gradient clipping\n",
    "    * Soluciones para los gradientes que desaparecen:\n",
    "        * Modificar las funciones de activación para evitar que el gradiente sea demasiado pequeño (o grande)\n",
    "        * Modificar la arquitectura para que las actualizaciones sean aditivas en lugar de multiplicativas\n",
    "        * Modificar la arquitectura para estandarizar las activaciones de cada capa, de manera que la distribución de las activaciones sobre los datos se mantenga contante durante el entrenamiento (sección 14.2.4 del PML)\n",
    "        * Escoger los valores iniciales de los parámetros cuidadosamente\n",
    "* [**Regularización:**](https://github.com/AlfonsJ/2024_01_31_RFA2324/tree/main/P03%20Redes%20neuronales%20profundas/T13%20Redes%20neuronales%20para%20datos%20tabulados/13.5%20Regularizaci%C3%B3n) $\\;$ veremos la sección 13.5 del PML\n",
    "    * Dropout suele dar buenos resultados  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
