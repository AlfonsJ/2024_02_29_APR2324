{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><table border-collapse: collapse><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=475>\n",
    "\n",
    "**2023_01_30_Problema 2.** $\\;$ El perceptrón multicapa de la figura se utiliza para resolver un problema de regresión.\n",
    "Se asume que la función de activación de los nodos de la capa de salida es de tipo lineal y de la capa oculta es de tipo sigmoid. Sean:\n",
    "* Vector de entrada: $\\;x_1=1.0,\\,x_2=0.0,\\,x_3=2.0$\n",
    "* Salidas de la capa oculta: $\\;s^1_1=0.622,\\,s^1_2=0.119$\n",
    "* Salidas de la capa de salida: $\\;s^2_1=-0.3145,\\,s^2_2=1.4304,\\,s^2_3=-0.9304$\n",
    "* Valores deseados de la capa de salida: $\\;t_1=-1.0,\\,t_2=1.0,\\,t_3=-2.0$\n",
    "\n",
    "Se pide:\n",
    "1. Los correspondientes errores en los tres nodos de la capa de salida y en los dos nodos de la capa oculta.\n",
    "2. Los nuevos valores de los pesos $\\theta^2_{32}$ y $\\theta^1_{23}$ asumiendo que el factor de aprendizaje $\\rho$ es $1.0$\n",
    "\n",
    "</td><td style=\"border: none; text-align:right; vertical-align:top; padding:0; margin:0;\" width=550>\n",
    "\n",
    "<img src=\"2023_01_30_Problema 2.png\" width=525>\n",
    "\n",
    "</td></tr></table></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** \n",
    "$$\\begin{align*}\n",
    "\\delta^2_1&=(t_1-s^2_1)=-0.6855%\n",
    "&\\quad\\delta^2_2&=(t_2-s^2_2)=-0.4304%\n",
    "&\\quad\\delta^2_3&=(t_3-s^2_3)=-1.0696\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\delta^1_1&=(\\delta^2_1~\\theta^2_{11}+\\delta^2_2~\\theta^2_{21}+\\delta^2_3~\\theta^2_{31})~s^1_1~(1-s^1_1)=0.3167\\\\%\n",
    "\\delta^1_2&=(\\delta^2_1~\\theta^2_{12}+\\delta^2_2~\\theta^2_{22}+\\delta^2_3~\\theta^2_{32})~s^1_2~(1-s^1_2)=-0.0049%\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\theta^2_{32}&=\\theta^2_{32}+\\rho~\\delta^2_3~s^1_2=(-1.0)+ (1)~(-1.0696)~(-0.9304)=-1.25499\\\\%\n",
    "\\theta^1_{23}&=\\theta^1_{23}+\\rho~\\delta^1_2~x_3=(-0.5)+(1)~(-0.0049)~(2.0)=-0.5195%\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><table border-collapse: collapse><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=400>\n",
    "\n",
    "**2023_01_16_Problema 2.** $\\;$ En la red de la figura, para un resolver un problema de regresión, se utilizan funciones de activación de tipo **sigmoid** en los nodos de la capa de salida y de la capa oculta y como factor de aprendizaje\n",
    "se ha escogido $\\rho=1.0$.\n",
    "\n",
    "Dados los pesos iniciales indicados en la figura, un vector de entrada $\\boldsymbol{x}^t=(1,1)$ y su valor deseado de salida $t=(0.1,0.9)$, calcular:\n",
    "1. Las salidas de todas las unidades.\n",
    "2. Los correspondientes errores en los nodos de la capa de salida y en los de la capa oculta.\n",
    "3. Los nuevos valores de los pesos de las conexiones al nodo 2 de la capa oculta.\n",
    "\n",
    "</td><td style=\"border: none; text-align:right; vertical-align:top; padding:0; margin:0;\" width=600>\n",
    "\n",
    "<img src=\"2023_01_16_Problema 2.png\" width=550>\n",
    "\n",
    "</td></tr></table></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "$$\\begin{align*}\n",
    "\\phi^1_1 &= \\theta^{1}_{10} + \\theta^{1}_{11} ~ x_1 + \\theta^{1}_{12} ~ x_2 = 3%\n",
    "&s^1_1 &= \\frac{1}{1+\\exp{(-\\phi^1_1})} = 0.953\\\\%\n",
    "\\phi^1_2 &= \\theta^{1}_{20} + \\theta^{1}_{21} ~ x_1 + \\theta^{1}_{22} ~ x_2 =-1%\n",
    "&s^1_2 &= \\frac{1}{1+\\exp{(-\\phi^1_2})} = 0.269\\\\%\n",
    "\\phi^2_1 &=\\theta^{2}_{10} +\\theta^{2}_{11} ~s^1_1+ \\theta^{2}_{12} ~s^1_2=2.221%\n",
    "&s^2_1 &= \\frac{1}{1+\\exp{(-\\phi^2_1})} = 0.902\\\\%\n",
    "\\phi^2_2 &= \\theta^{2}_{20} +\\theta^{2}_{21} ~s^1_1 +\\theta^{2}_{22} ~s^1_2=-0.222%\n",
    "&s^2_2 &= \\frac{1}{1+\\exp{(-\\phi^2_2})} = 0.445\\\\[1em]%\n",
    "\\delta^2_1 &= (t_1-s^2_1)~s^2_1~(1-s^2_1) = -0.0708%\n",
    "&\\delta^2_2 &= (t_2-s^2_2)~s^2_2~(1-s^2_2) = 0.1124\\\\[1em]\n",
    "\\delta^1_1 &= (\\delta^2_1~\\theta^{2}_{11} + \\delta^2_2~\\theta^{2}_{21})~s^1_1~(1-s^1_1) =-0.0082%\n",
    "&\\delta^1_2 &= (\\delta^2_1~\\theta^{2}_{12} + \\delta^2_2~\\theta^{2}_{22})~s^1_2~(1-s^1_2) =-0.0360\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\theta^{1}_{20} &= \\theta^{1}_{20}+\\rho~\\delta^1_2~(+1)=0.964\\\\\n",
    "\\theta^{1}_{21} &= \\theta^{1}_{21}+\\rho~\\delta^1_2~x_1=-1.036\\\\\n",
    "\\theta^{1}_{22} &= \\theta^{1}_{22}+\\rho~\\delta^1_2~x_2=-1.036\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><table border-collapse: collapse><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=550>\n",
    "\n",
    "**2022_01_24_Problema 2.** $\\;$ La red hacia adelante (\"feedforward\") de la figura se utiliza para resolver un problema de regresión, con función de activación de los nodos de la capa de oculta de tipo **sigmoid** y lineal en el nodo de la capa de salida, y factor de aprendizaje $\\rho=1.0$.\n",
    "\n",
    "Dados unos pesos iniciales $\\,\\theta_{1}=\\theta_{2}=\\theta_{3}=\\theta_{4}=\\theta_{5}=1.0,\\,$ un vector de entrada $\\boldsymbol{x}^t=(0,1)$ y su valor deseado de salida $t=1,\\,$ calcula:\n",
    "1. Las salidas de todas las unidades.\n",
    "2. Los correspondientes errores en el nodo de la capa de salida y en el de la capa oculta.\n",
    "3. Los nuevos valores de los pesos de las conexiones.\n",
    "\n",
    "*Pista:* La actualización de pesos en esta red sigue la misma formulación que en el BackProp para el perceptrón multicapa\n",
    "convencional: el incremento de peso es $\\Delta\\theta~=~\\rho~z~\\delta$, donde $\\rho$ es el factor de aprendizaje, $z$ es la entrada del arco asociado al peso $\\theta$, y $\\delta$ es el error que se observa en la salida de la unidad a la que llega ese arco, multiplicado por la derivada de la función de activación.\n",
    "\n",
    "</td><td style=\"border: none; text-align:right; vertical-align:top; padding:0; margin:0;\" width=450>\n",
    "\n",
    "<img src=\"2022_01_24_Problema 2.png\" width=400>\n",
    "\n",
    "</td></tr></table></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "$$\\begin{align*}\n",
    "\\phi^1_1 &= \\theta_{1} + \\theta_{2}x_2 = 1+ 1\\cdot 1 =2%\n",
    "&s^1_1&=\\frac{1}{1+\\exp{(-\\phi^1_1})} ~=~ 0.880797\\\\%\n",
    "\\phi^2_1 &= \\theta_{4} + \\theta_{3} x_1 + \\theta_{5} s^1_1 = 1+1\\cdot 0+1\\cdot 0.880797=1.880797%\n",
    "&s^2_1&= \\phi^2_1 = 1.880797\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\delta^2_1&=(t_1-s^2_1)=(1-1.880797)=-0.880797\\\\%\n",
    "\\delta^1_1&=(\\delta^2_1~\\theta_{5})s^1_1~(1-s^1_1)=(-0.880797\\cdot 1)\\cdot 0.880797\\cdot (1-0.880797)=-0.092478%\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\theta_{1}&=\\theta_{1}+\\rho\\delta^1_1(+1)=1+1\\cdot(-0.092478)\\cdot 1=0.90752\\\\%\n",
    "\\theta_{2}&=\\theta_{2}+\\rho\\delta^1_1x_2=1+1\\cdot(-0.092478)\\cdot 1=0.90752\\\\%\n",
    "\\theta_{3}&=\\theta_{3}+\\rho\\delta^2_1x_1=1+1\\cdot(-0.880797)\\cdot 0=1\\\\%\n",
    "\\theta_{4}&=\\theta_{4}+\\rho\\delta^2_1(+1)=1+1\\cdot(-0.880797)\\cdot 1=0.119203\\\\%\n",
    "\\theta_{5}&=\\theta_{5}+\\rho\\delta^2_1s^1_1=1+1\\cdot(-0.880797)\\cdot 0.880797=0.224196%\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
